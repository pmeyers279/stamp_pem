#!/usr/bin/env python

from stamp_pem import coh_io
from glue import pipeline
import optparse
from gwpy.segments import DataQualityFlag
import os


def parse_command_line():
    """
    parse command parse command line
    """
    parser = optparse.OptionParser()
    parser.add_option(
        "--ini-file", "-i", help="pipeline ini file",
        default=None, type=str, dest='ini')
    parser.add_option(
        "-s", "--start-time", help="start time", default=None,
        type="int", dest="st")
    parser.add_option(
        "-e", "--end-time", help="end time", default=None,
        type="int", dest="et")
    parser.add_option(
        "--num-chans", help="number of channels",
        dest="nchans", type=int, default=20)

    params, args = parser.parse_args()
    return params


def build_arg(env_params, run_params):
    flags = {}
    flags['-list'] = env_params['list']
    flags['s'] = '$(st)'
    flags['e'] = '$(et)'
    flags['-fhigh'] = str(run_params['fhigh'])
    flags['-darm-channel'] = run_params['darm_channel']
    flags['-stride'] = str(run_params['stride'])
    flags['-subsystem'] = '$(subsystem)'
    flags['-segment-duration'] = str(run_params['segmentduration'])
    flags['d'] = str(env_params['base_directory'])
    flags['-frames'] = 1
    if run_params['spec_fhigh']:
        flags['-spec-fhigh'] = str(run_params['spec_fhigh'])
    if run_params['spec_flow']:
        flags['-spec-flow'] = str(run_params['spec_flow'])
    arg = ''
    for flag in flags.keys():
        arg = arg + ' -%s %s' % (flag, flags[flag])
    return arg


params = parse_command_line()

pipeline_dict = coh_io.read_pipeline_ini(params.ini)
env_params, run_params = coh_io.check_ini_params(pipeline_dict)
channel_dict = coh_io.read_list(env_params['list'])
try:
    time_file = env_params['online_time_file']
    inc = int(env_params['online_increment'])
    st = coh_io.read_time_from_file(time_file)
    et = st + inc
except KeyError:
    st = params.st
    et = params.et

# create directory structure
subsystems = channel_dict.keys()
coh_io.create_directory_structure(subsystems, st, env_params['base_directory'])

# get darm and DQ flag from params...we'll use them a lot.
darm_channel = run_params['darm_channel']
flag = run_params['flag']


# make sure flag and darm are for same IFO. I've done this wrong
# too many times to make the mistake again.
if not coh_io.check_channel_and_flag(darm_channel, flag):
    raise ValueError('channel and flag are not for same IFO!')

# get and write DQ segments:
segs = DataQualityFlag.query_dqsegdb(
    flag, st, et, url='https://segments.ligo.org')
seg_dir = coh_io.get_directory_structure(
    'SEGMENTS', st, env_params['base_directory'])
seg_file = coh_io.create_coherence_data_filename(flag, 'SEGMENTS', st, et,
                                                 directory=seg_dir)
segs.write('%s.xml.gz' % (seg_file))


datajob = pipeline.CondorDAGJob('vanilla', env_params['executable'])
datajob2 = pipeline.CondorDAGJob(
    'vanilla', env_params['combine_executable'])
dag = pipeline.CondorDAG(
    '/usr1/%s/$(subsystem).log' % (env_params['user']))
coh_io.create_directory_structure(
    channel_dict.keys(), st, directory=env_params['base_directory'])

dag_dir = coh_io.get_directory_structure(
    'DAGS', st, env_params['base_directory'])
sub_node = {}
for subsystem in channel_dict.keys():
    chans = []
    for key in channel_dict[subsystem].keys():
        chans.append(channel_dict[subsystem][key])
        njobs = ceil(len(chans) / params.nchans)
    for seg in segs.active:
        seg_st = seg[0].seconds
        seg_et = seg[1].seconds
        for njob in njobs:
            job = pipeline.CondorDAGJob('vanilla', env_params['executable'])
            job.set_sub_file('%s/%s-%d-%d.sub' % (
                             dag_dir, darm_channel.replace(':', '-'),
                             seg_st, seg_et))
            job.set_stderr_file(
                '%s/$(subsystem).err' % (dag_dir))
            job.set_stdout_file(
                '%s/$(subsystem).out' % (dag_dir))
            node = pipeline.CondorDAGNode(job)
            node.add_macro("subsystem", subsystem)
            node.add_macro("st", seg_st)
            node.add_macro("et", seg_et)
            node.add_macro('--job-num', njob)
            sub_node[subsystem] = node
            dag.add_node(node)
for subsystem in channel_dict.keys():
    job = pipeline.CondorDAGJob(
        'vanilla', env_params['combine_executable'])
    job.set_sub_file('%s/combine_jobs-%d-%d.sub' % (dag_dir, st, et))
    job.set_stderr_file(
        '%s/$(subsystem)-combine.err' % dag_dir)
    job.set_stdout_file(
        '%s/$(subsystem)-combine.out' % dag_dir)
    node = pipeline.CondorDAGNode(job)
    node.add_macro('subsystem', subsystem)
    node.add_parent(sub_node[subsystem])
    dag.add_node(node)

dagName = '%s/%s-%d-%d' % (
    dag_dir, darm_channel.replace(':', '-'), st, et)

# datajob info
datajob_sub = '%s/%s-%d-%d.sub' % (
    dag_dir, darm_channel.replace(':', '-'), st, et)
datajob.set_sub_file(datajob_sub)

datajob2_sub = '%s/combine_jobs-%d-%d.sub' % (dag_dir, st, et)
datajob.set_stderr_file(
    '%s/$(subsystem).err' % (dag_dir))
datajob.set_stdout_file(
    '%s/$(subsystem).out' % (dag_dir))
datajob.set_log_file(
    '%s/$(subsystem).log' % (dag_dir))

# combine jobs post processing info
datajob2.set_sub_file(datajob2_sub)
datajob2.set_stderr_file(
    '%s/$(subsystem)-combine.err' % (dag_dir))
datajob2.set_stdout_file(
    '%s/$(subsystem)-combine.out' % (dag_dir))
datajob2.set_log_file(
    '/usr1/%s/$(subsystem)-combine.log' % env_params['user'])
arg = build_arg(env_params, run_params)
print 'ARG = %s' % arg
datajob.add_arg(arg)
datajob2.add_arg('-s %d -e %d --subsystem $(subsystem) --darm-channel %s --flag %s --directory %s' % (
                 st, et, darm_channel, flag, env_params['base_directory']))
datajob.add_condor_cmd('getEnv', 'True')
datajob2.add_condor_cmd('getEnv', 'True')
datajob.add_condor_cmd('accounting_group', env_params['accounting_tag'])
datajob2.add_condor_cmd('accounting_group', env_params['accounting_tag'])
datajob.add_condor_cmd('accounting_group_user', env_params['accounting_user'])
datajob2.add_condor_cmd('accounting_group_user', env_params['accounting_user'])
datajob.write_sub_file()
datajob2.write_sub_file()
dag.set_dag_file(dagName)
dag.write_dag()
